{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4037e31c-03ba-4af4-bc08-931eeb71910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from skimage import io, transform\n",
    "import math\n",
    "from math import * \n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.transform import rotate as rotate_transform\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255751d6-adaf-47f4-b7bc-f500abbfb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rotate(self, image, params):\n",
    "\n",
    "        angle = params['rotation_range'][0]\n",
    "        angle = (random.uniform(0,1))*random.choice([-1,1])*angle\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = rotate_transform(np.array(image), angle = angle, mode = 'edge')\n",
    "\n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "    def translation(self, image,  params):\n",
    "        image_shape = np.array(image).shape\n",
    "        ty = random.uniform(params['height_shift_range'][0]*image_shape[0],          \n",
    "                            params['height_shift_range'][1]*image_shape[0])\n",
    "        tx = random.uniform(params['width_shift_range'][0]*image_shape[1],\n",
    "                            params['width_shift_range'][1]*image_shape[1] )\n",
    "\n",
    "        \n",
    "        horizontal_shift =  tx*random.choice([-1,1])\n",
    "        vertical_shift = ty*random.choice([-1,1])\n",
    "        horizontal_shift_normalised = horizontal_shift/image_shape[1]\n",
    "        vertical_shift_normalised =  vertical_shift/image_shape[0]\n",
    "\n",
    "        transform = AffineTransform(translation=(-horizontal_shift,-vertical_shift))\n",
    "\n",
    "        image = warp(np.array(image),transform,mode='edge')\n",
    "\n",
    "\n",
    "  \n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "    def resize(self, image, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image\n",
    "\n",
    "    def zoom(self, image, params):\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        zoom = random.uniform(params['zoom_range'][0],params['zoom_range'][1])\n",
    "        image = TF.resize(image,(int(img_shape[0]*zoom), int(img_shape[1]*zoom)) )\n",
    "        scale_transform = torch.tensor([[zoom, 0], \n",
    "                                        [0, zoom]])\n",
    "\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def color_jitter(self, image):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image\n",
    "    \n",
    "    def __call__(self, image, params, image_size):\n",
    "\n",
    "        # set checked image and landmark to landmark_ and image_ (this is for making sure we use the last checked tranformed instead of wrongly tranformed to do the following               # tranform)\n",
    "        \n",
    "        # -----------------------\n",
    "        image_ = Image.fromarray(image.copy())\n",
    "\n",
    "        # -----------------------\n",
    "\n",
    "        # ZOOM\n",
    "        image  = self.zoom(image_,  params)\n",
    "        \n",
    "\n",
    "        # RESIZE\n",
    "\n",
    "        image = self.resize(image, (image_size, image_size))\n",
    "\n",
    "        # ----------------------\n",
    "        #image_, landmarks_ = self.color_jitter(image_, landmarks_)\n",
    "        # ----------------------\n",
    "        \n",
    "        # ROTATE\n",
    "        image = self.rotate(image,  params)\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "        image = image\n",
    "        # ----------------------\n",
    "\n",
    "        # TRANSLATION\n",
    "        image= self.translation(image, params)\n",
    "\n",
    " \n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1d5a42-bcef-4da5-abac-99af92a8cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset():\n",
    "\n",
    "    def __init__(self, transform=None,zoom = [1.0 - 0.03258157476873315, 1.0 + 0.03258157476873315], rotation = [22], height_shift= [0,0.03003200603616672], width_shift= [0,0.03003200603616672 ]):\n",
    "\n",
    "        # targets 0\n",
    "        filenames1 = os.listdir('C:/Users/19480105/OneDrive - Stellenbosch University/Documents/1. Research Project/Data/tier1_training_set/Missing_landmarkwings_L/')\n",
    "        # targets 1\n",
    "        filenames3 = os.listdir('C:/Users/19480105/OneDrive - Stellenbosch University/Documents/1. Research Project/Data/tier1_training_set/goodwingsv20-21/')\n",
    "    \n",
    "        self. tranform = transform\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "        self.image_filenames = []\n",
    "        self.targets = []\n",
    "        self.image_size = 224\n",
    "        self.transform = transform\n",
    "        self.image_dir = 'C:/Users/19480105/OneDrive - Stellenbosch University/Documents/1. Research Project/Data/tier1_training_set/Missing_landmarkwings_L/'\n",
    "        \n",
    "        self.image_dir3 = 'C:/Users/19480105/OneDrive - Stellenbosch University/Documents/1. Research Project/Data/tier1_training_set/goodwingsv20-21/'\n",
    "        self.TransF_ = True\n",
    "\n",
    "       # ------------------- Append left wings data to dataset class ------------\n",
    "\n",
    "        for filename in filenames1:\n",
    "            self.image_filenames.append(os.path.join(self.image_dir, filename))\n",
    "            self.targets.append(1)\n",
    "\n",
    "            \n",
    "\n",
    "        # ------------------ Append flipped right wings data to dataset class-----\n",
    "\n",
    "\n",
    "        #for filename in filenames2[:]:\n",
    "        #    self.mirror(image)\n",
    "        #    self.targets.append(1)\n",
    "        #    self.image_filenames.append(os.path.join(self.image_dir2, filename))\n",
    "\n",
    "        #num = len(self.targets.copy())\n",
    "        for filename in filenames3:\n",
    "            self.targets.append(0)\n",
    "            self.image_filenames.append(os.path.join(self.image_dir3, filename))\n",
    "\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "    def TransF(self):\n",
    "        self.TransF_ = True\n",
    "    def NoTransF(self):\n",
    "        self.TransF_ = False\n",
    "    def resize(self,size):\n",
    "        self.image_size = size\n",
    "    def set_params(self, zoom = [0.95, 0.105], rotation = [10], height_shift= [0,0.05], width_shift= [0,0.05]):\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        params = {'zoom_range': self.zoom, 'rotation_range':self.rotation, 'height_shift_range': self.height_shift, 'width_shift_range': self.width_shift }\n",
    "        image_ = plt.imread(self.image_filenames[index])\n",
    "        target = torch.tensor(self.targets[index])\n",
    "\n",
    "        image = plt.imread(self.image_filenames[index])\n",
    "\n",
    "        \n",
    "        if self.transform and self.TransF_:\n",
    "            \n",
    "            image = self.transform(image_, params, self.image_size)\n",
    "\n",
    "        else:\n",
    "            img_shape = image.copy().shape\n",
    "            image = Image.fromarray(image)\n",
    "            image = TF.resize(image, (self.image_size,self.image_size))\n",
    "       \n",
    "            image = TF.to_tensor(image)\n",
    "            # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "            image = TF.normalize(image, [0.5], [0.5])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "DataSet = LandmarksDataset(Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b182be4a-5dbf-4bb9-83ac-d8593e71fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_bn_(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super().__init__()\n",
    "        self.model_name='vgg16_bn'\n",
    "        self.model=models.vgg16_bn(pretrained=True)\n",
    "        #self.model.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.classifier=nn.Linear(self.model.classifier[0].in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.model(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b975dea3-2a26-4d34-9a8a-c432b9c3f1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nuhrr\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nuhrr\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vgg16_bn_(\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (36): ReLU(inplace=True)\n",
       "      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (39): ReLU(inplace=True)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Linear(in_features=25088, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_bn_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af97c77-23b0-4cbf-866f-26f7273e63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    return(sum((predictions.round() == y)) / float(len(y))).item()\n",
    "\n",
    "\n",
    "    # helper functions\n",
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.6f \" % (step, total_step, loss))   \n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.6f \" % (step, total_step, loss))\n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d45f18-51f5-4a33-a67b-2141234a06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Train set is 720\n",
      "The length of Valid set is 240\n",
      "The length of Valid set is 240\n"
     ]
    }
   ],
   "source": [
    "DataSet.TransF()\n",
    "DataSet.resize(224)\n",
    "dataset = DataSet\n",
    "# split the dataset into validation and test sets\n",
    "len_valid_test_set = int(0.2*len(dataset)) # 60% training, 20% validation, 20% testing\n",
    "\n",
    "len_train_set = len(dataset) - len_valid_test_set*2\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "\n",
    "train_dataset , valid_dataset, test_dataset  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_test_set, len_valid_test_set], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=15, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b353a74e-899f-4d48-b3f1-ef8874925954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Steps: 16/16  Loss: 0.045525 \n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train Loss: 0.2292  Valid Loss: 0.0455\n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train acc: 0.91250  Valid acc: 0.98750\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0455 at epoch 1/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 16/16  Loss: 0.008297 \n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train Loss: 0.0326  Valid Loss: 0.0083\n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train acc: 0.99167  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0083 at epoch 2/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 16/16  Loss: 0.004669 \n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train Loss: 0.0149  Valid Loss: 0.0047\n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train acc: 0.99583  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0047 at epoch 3/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 16/16  Loss: 0.107020 \n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train Loss: 0.0089  Valid Loss: 0.1070\n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train acc: 0.99583  Valid acc: 0.97500\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.029075 \n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train Loss: 0.0202  Valid Loss: 0.0291\n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train acc: 0.99444  Valid acc: 0.98750\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.017598 \n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train Loss: 0.0096  Valid Loss: 0.0176\n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train acc: 0.99722  Valid acc: 0.99167\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.025994 \n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train Loss: 0.0261  Valid Loss: 0.0260\n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train acc: 0.99444  Valid acc: 0.98750\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.040986 \n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train Loss: 0.0127  Valid Loss: 0.0410\n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train acc: 0.99583  Valid acc: 0.98333\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.003387 \n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train Loss: 0.0018  Valid Loss: 0.0034\n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0034 at epoch 9/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 16/16  Loss: 0.015783 \n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train Loss: 0.0013  Valid Loss: 0.0158\n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.041300 \n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train Loss: 0.0362  Valid Loss: 0.0413\n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train acc: 0.99028  Valid acc: 0.99167\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.027215 \n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train Loss: 0.0071  Valid Loss: 0.0272\n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train acc: 0.99722  Valid acc: 0.99167\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.007167 \n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train Loss: 0.0021  Valid Loss: 0.0072\n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.009426 \n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train Loss: 0.0023  Valid Loss: 0.0094\n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.020928 \n",
      "--------------------------------------------------\n",
      "Epoch: 15  Train Loss: 0.0008  Valid Loss: 0.0209\n",
      "--------------------------------------------------\n",
      "Epoch: 15  Train acc: 1.00000  Valid acc: 0.99167\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.014558 \n",
      "--------------------------------------------------\n",
      "Epoch: 16  Train Loss: 0.0016  Valid Loss: 0.0146\n",
      "--------------------------------------------------\n",
      "Epoch: 16  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.016230 \n",
      "--------------------------------------------------\n",
      "Epoch: 17  Train Loss: 0.0081  Valid Loss: 0.0162\n",
      "--------------------------------------------------\n",
      "Epoch: 17  Train acc: 0.99722  Valid acc: 0.98750\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.064418 \n",
      "--------------------------------------------------\n",
      "Epoch: 18  Train Loss: 0.0153  Valid Loss: 0.0644\n",
      "--------------------------------------------------\n",
      "Epoch: 18  Train acc: 0.99444  Valid acc: 0.98333\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.003923 \n",
      "--------------------------------------------------\n",
      "Epoch: 19  Train Loss: 0.0068  Valid Loss: 0.0039\n",
      "--------------------------------------------------\n",
      "Epoch: 19  Train acc: 0.99861  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.006242 \n",
      "--------------------------------------------------\n",
      "Epoch: 20  Train Loss: 0.0005  Valid Loss: 0.0062\n",
      "--------------------------------------------------\n",
      "Epoch: 20  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.012598 \n",
      "--------------------------------------------------\n",
      "Epoch: 21  Train Loss: 0.0125  Valid Loss: 0.0126\n",
      "--------------------------------------------------\n",
      "Epoch: 21  Train acc: 0.99861  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.004742 \n",
      "--------------------------------------------------\n",
      "Epoch: 22  Train Loss: 0.0025  Valid Loss: 0.0047\n",
      "--------------------------------------------------\n",
      "Epoch: 22  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.004944 \n",
      "--------------------------------------------------\n",
      "Epoch: 23  Train Loss: 0.0004  Valid Loss: 0.0049\n",
      "--------------------------------------------------\n",
      "Epoch: 23  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.003558 \n",
      "--------------------------------------------------\n",
      "Epoch: 24  Train Loss: 0.0013  Valid Loss: 0.0036\n",
      "--------------------------------------------------\n",
      "Epoch: 24  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.008767 \n",
      "--------------------------------------------------\n",
      "Epoch: 25  Train Loss: 0.0007  Valid Loss: 0.0088\n",
      "--------------------------------------------------\n",
      "Epoch: 25  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.007650 \n",
      "--------------------------------------------------\n",
      "Epoch: 26  Train Loss: 0.0011  Valid Loss: 0.0076\n",
      "--------------------------------------------------\n",
      "Epoch: 26  Train acc: 1.00000  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.020208 \n",
      "--------------------------------------------------\n",
      "Epoch: 27  Train Loss: 0.0014  Valid Loss: 0.0202\n",
      "--------------------------------------------------\n",
      "Epoch: 27  Train acc: 1.00000  Valid acc: 0.99167\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.308611 \n",
      "--------------------------------------------------\n",
      "Epoch: 28  Train Loss: 0.0158  Valid Loss: 0.3086\n",
      "--------------------------------------------------\n",
      "Epoch: 28  Train acc: 0.99583  Valid acc: 0.92500\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.008759 \n",
      "--------------------------------------------------\n",
      "Epoch: 29  Train Loss: 0.0165  Valid Loss: 0.0088\n",
      "--------------------------------------------------\n",
      "Epoch: 29  Train acc: 0.99306  Valid acc: 0.99583\n",
      "--------------------------------------------------\n",
      "Valid Steps: 16/16  Loss: 0.003356 \n",
      "--------------------------------------------------\n",
      "Epoch: 30  Train Loss: 0.0073  Valid Loss: 0.0034\n",
      "--------------------------------------------------\n",
      "Epoch: 30  Train acc: 0.99444  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0034 at epoch 30/30\n",
      "Model Saved\n",
      "\n",
      "Training Complete\n",
      "Total Elapsed Time : 18228.00305247307 s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(predictions)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#torch.empty_cache() #torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(results,f)\n\u001b[0;32m    115\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(predictions)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#torch.empty_cache() #torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(results,f)\n\u001b[0;32m    115\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl'"
     ]
    }
   ],
   "source": [
    "# feature transfer learning (none of th weights are frozen)\n",
    "\n",
    "results = {'acc_train': [], 'acc_val': [], 'loss_train': [], 'loss_val':[], 'time': []}\n",
    "network = vgg16_bn_()\n",
    "\n",
    "# TRAIN\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.0001)\n",
    "loss_min = np.inf\n",
    "num_epochs = 30\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    acc_train = 0\n",
    "    acc_valid = 0\n",
    "    running_acc = 0\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "\n",
    "        images, targets = next(iter(train_loader))\n",
    "        targets = targets.float()\n",
    "    \n",
    "        predictions = network(images).flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        # find the loss for the current step\n",
    "    \n",
    "        loss_train_step = criterion(predictions, targets)\n",
    "\n",
    "\n",
    "        acc_train_step = accuracy(predictions, targets)\n",
    "\n",
    "\n",
    "        \n",
    "        # calculate the gradients\n",
    "\n",
    "        loss_train_step.backward()\n",
    "    \n",
    "        \n",
    "        # update the parameters\n",
    "\n",
    "        optimizer.step()\n",
    "        acc_train += acc_train_step\n",
    "        loss_train += loss_train_step.item()\n",
    "        \n",
    "        running_acc  = acc_train/step\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            \n",
    "            images, targets = next(iter(valid_loader))\n",
    "            targets = targets.float()\n",
    "        \n",
    "            predictions = network(images).flatten()\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, targets)\n",
    "\n",
    "            acc_valid_step = accuracy(predictions, targets)\n",
    "\n",
    "            acc_valid += acc_valid_step\n",
    "            running_acc = acc_valid/step\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    acc_train /= len(train_loader)\n",
    "    acc_valid /= len(valid_loader)\n",
    "    \n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    print('Epoch: {}  Train acc: {:.5f}  Valid acc: {:.5f}'.format(epoch, acc_train, acc_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    results['loss_train'].append(loss_train)\n",
    "    results['loss_val'].append(loss_valid)\n",
    "    results['acc_train'].append(acc_train)\n",
    "    results['acc_val'].append(acc_valid)\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/model_vgg16_bn_classifer_finetune.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "results['time'].append(time.time()-start_time)\n",
    "del(network)\n",
    "del(images)\n",
    "del(targets)\n",
    "del(predictions)\n",
    "#torch.empty_cache() #torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805a4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:/Users/19480105/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_finetune_trainingdata.pkl\",\"wb\")\n",
    "pickle.dump(results,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d1389-dccf-402b-b280-07bd0d3f1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature transfer learning (weights are frozen)\n",
    "\n",
    "results = {'acc_train': [], 'acc_val': [], 'loss_train': [], 'loss_val':[], 'time': []}\n",
    "network = vgg16_bn_()\n",
    "for param in network.model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "# TRAIN\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(network.parameters(), )\n",
    "loss_min = np.inf\n",
    "num_epochs = 50\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    acc_train = 0\n",
    "    acc_valid = 0\n",
    "    running_acc = 0\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "\n",
    "        images, targets = next(iter(train_loader))\n",
    "        images = images.float()\n",
    "        targets = targets.float()\n",
    "    \n",
    "        predictions = network(images).flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        # find the loss for the current step\n",
    "    \n",
    "        loss_train_step = criterion(predictions, targets)\n",
    "\n",
    "\n",
    "        acc_train_step = accuracy(predictions, targets)\n",
    "\n",
    "\n",
    "        \n",
    "        # calculate the gradients\n",
    "\n",
    "        loss_train_step.backward()\n",
    "    \n",
    "        \n",
    "        # update the parameters\n",
    "\n",
    "        optimizer.step()\n",
    "        acc_train += acc_train_step\n",
    "        loss_train += loss_train_step.item()\n",
    "        \n",
    "        running_acc  = acc_train/step\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            \n",
    "            images, targets = next(iter(valid_loader))\n",
    "            images = images.float()\n",
    "            targets = targets.float()\n",
    "        \n",
    "            predictions = network(images).flatten()\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, targets)\n",
    "\n",
    "            acc_valid_step = accuracy(predictions, targets)\n",
    "\n",
    "            acc_valid += acc_valid_step\n",
    "            running_acc = acc_valid/step\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    acc_train /= len(train_loader)\n",
    "    acc_valid /= len(train_loader)\n",
    "    \n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    results['loss_train'].append(loss_train)\n",
    "    results['loss_train'].append(loss_valid)\n",
    "    results['acc_train'].append(acc_train)\n",
    "    results['acc_val'].append(acc_valid)\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), 'C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/model_vgg16_bn_classifer_fixedfeatures.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "results['time'].append(time.time()-start_time)\n",
    "del(network)\n",
    "del(images)\n",
    "del(targets)\n",
    "del(predictions)\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "f = open(\"C:/Users/nuhrr/OneDrive - Stellenbosch University/Documents/1. Research Project/Models/training_losses/model_vgg16_bn_classifer_fixedfeatures_trainingdata.pkl\",\"wb\")\n",
    "pickle.dump(results,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
